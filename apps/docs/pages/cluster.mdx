import { Callout } from 'nextra/components';
import { Tab, Tabs } from 'nextra-theme-docs';
import Image from 'next/image';

# Cluster

One of the big challenges in consolidating multiple data sources is finding clusters that succinctly describe multiple grants from different sources. For example, there might be 3 grants in total for saving whales, but each one is from a different source with different labels/categories and a completely different format. How do we know that these 3 grants are actually about the same thing? We mostly bypass this problem by implementing semantic search as explained in the [Search section](/search). Although a text-based search is good, deterministic filters on data can help to narrow down the search space and make the experience of finding funding a lot more successful. For example, if we know that the grant is about saving whales, we can filter out all the grants that are not about saving whales.

Everything that we talk about in this section is still work-in-progress and we are still experimenting with different approaches.

<Callout>
  You can follow all the steps in the [Jupyter
  Notebook](https://github.com/Code42Cate/funding/blob/main/clustering.ipynb) as well!
</Callout>

## Data Preparation

The first step is to prepare the data. To do that, follow the tutorial in the [Setup](/setup) section. Once you have your scraped data in your database, come back here and continue.

Create a new folder in the root directory of this repositiry called `data`. This is where you will put your datasets.

You will run 3 separate queries for each data source. First, run:

```sql copy
SELECT title, url, embedding FROM funding_opportunities WHERE type = 'FOERDERDATENBANK'
```

And export the result as CSV file with the name `funding_foerderdatenbank.csv` to the `data` folder.

Make sure to add a header row, escape quotes, and use `,` as the delimiter.

Then do the same for the following queries:

```sql copy
SELECT title, url, embedding FROM funding_opportunities WHERE type = 'DAAD'
```

Filename: `funding_daad.csv`

```sql copy
SELECT title, url, embedding FROM funding_opportunities WHERE type = 'EU'
```

Filename: `funding_eu.csv`

Each CSV file should have look similar to this:

```csv
"title","url","embedding"
"Increased Cybersecurity 2024","https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/horizon-cl3-2024-cs-01-01","[0.010429002,-0.029917268,0.011168119,-0.026933677,-0.0020071422,0.0033717954]"
"Implementing the European Open Science Cloud","https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/infraeosc-03-2020","[-0.004084645,-0.017044233]"
"Research and Innovation and other actions to support the implementation of mission A Soil Deal for Europe","https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/horizon-miss-2023-soil-01-06","[-0.009423625,-0.0018956903]"
```

## Determining the number of clusters

The clustering algorithm we are using needs to know how many clusters to create. To determine this, we use the [Silhouette method](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Silhouette_method). The Silhouette measures how closely a data point is related to all datapoints within its own cluster, compared to all datapoints in the neighbouring clusters. If each data point is very closely related to its own cluster and very distantly related to the neighbouring cluster, then the Silhouette will be close to 1. This can then be interpreted as a good clustering!

Before we can calculate the Silhouette scores for different amounts of clusters, we need to set a upper limit for the number of clusters to try out. This is mostly guess work, but we can use our requirement that each cluster should have at least 3 grants to set an upper bound of $\lfloor \frac{n}{3} \rfloor$ where $n$ is the total number of grants. If we realize that most clusters are too specific, we can always decrease this number later.

Plotting the Silhouette scores for different numbers of clusters gives us the following graph:

<Image src="/silhouette.png" alt="Silhouette Scores" width={300} height={300} />

The x-axis represents the number of clusters, and the y-axis represents the Silhouette score. Using this graph we can see that the Silhouette score is highest for 64 clusters! [^1]

[^1]: This is just an example, the actual number of clusters is different and are highly dependent on your data.

## Clustering

We use a combination of [k-means](https://en.wikipedia.org/wiki/K-means_clustering) to cluster the data and [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to visualize the clusters. The following is a brief explanation of how these algorithms work, skip this if you are already familiar with them.

K-means is a popular unsupervised machine learning algorithm used for clustering data. Given a dataset, k-means aims to partition the data into k clusters, where each data point belongs to the cluster with the nearest mean. The algorithm iteratively updates the cluster centroids and reassigns data points to clusters until convergence, aiming to minimize the sum of squared distances between data points and their respective cluster centroids. We figured out k with the Silhouette method in the previous step. [Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) provides the implemenation of our algorithm.

[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding), a dimensionality reduction technique, is valuable for visualizing and exploring high-dimensional data, including long texts. By preserving local structure and capturing non-linear relationships, it suits the complexity of text data. When clustering long texts, [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is often combined with k-mean.

```python
kmeans = KMeans(n_clusters=n_clusters, init="k-means++", random_state=42)
kmeans.fit(matrix)
labels = kmeans.labels_
embedding_df["Cluster"] = labels

tsne = TSNE(
    n_components=2, perplexity=15, random_state=42, init="random", learning_rate=200
)
vis_dims2 = tsne.fit_transform(matrix)

x = [x for x, y in vis_dims2]
y = [y for x, y in vis_dims2]

for category, color in enumerate(mcolors.CSS4_COLORS):
    xs = np.array(x)[embedding_df.Cluster == category]
    ys = np.array(y)[embedding_df.Cluster == category]
    plt.rcParams["figure.figsize"] = (50, 50)

    plt.scatter(xs, ys, color=color, alpha=0.3)
    plt.scatter(xs.mean(), ys.mean(), marker="x", color=color, s=100, edgecolors="black")

```

While the resulting visualization is nice to look at, they do not provide any information about _why_ they are clustered together. To solve this problem, we iterate through the clusters and let a gpt-3 model summarize the similarities between the grants in each cluster.

## Visualization

The following shows a 2D visualization of the different datasets. This is mostly for aesthetic purposes, but it can also help to get a better understanding of the data. The visualization is a basic scatter plot done with Matplotlib, where each point represents a grant. The colors are assigned randomly.

<Tabs items={['Foerderdatenbank', 'EU', 'DAAD']}>
  <Tab>
    <Image src="/fd.png" alt="Foerderdatenbank Cluster Visualization" width={1000} height={1000} />
  </Tab>
  <Tab>
    <Image src="/eu.png" alt="Foerderdatenbank Cluster Visualization" width={1000} height={1000} />
  </Tab>
  <Tab>
    <Image src="/daad.png" alt="Foerderdatenbank Cluster Visualization" width={1000} height={1000} />
  </Tab>
</Tabs>

## Conclusion

Although the clusters are not perfect, they provide a good starting point for further exploration. One issue that we identified is that the number of clusters that is needed to be sufficiently specific is too high to be displayed in a web interface. We are currently experimenting with different ways to solve this problem, both on the clustering and UX side.

## Next Steps

Everything up until this point was a completely manual process - but the data changes often, and so do the clusters. The next challenge is automating this process so that we can keep the clusters up to date. For that, we especially need to find a way to do proper quality assurance on the results, since the output of both the clustering and summarization models is not always perfect.

---
