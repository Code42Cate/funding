import Image from 'next/image';

# Architecture

diagram:

## Repository Structure

The repository is a monorepo using [NX](https://nx.dev/) and is structured as follows:

```
apps
├─> docs (next.js app, available under https://docs.funding.nplusone.studio)
├─> scraper (Node.js app for scraping data)
├─> web (next.js app, available under https://funding.nplusone.studio)
└─> web-e2e (cypress e2e tests for the web app)
libs
├─> daad-scraper (Node.js library for scraping DAAD data, part of scraper app)
├─> eu-scraper (Node.js library for scraping EU data, part of scraper app)
├─> foerderdatenbank-scraper (Node.js library for scraping Foerderdatenbank data, part of scraper app)
├─> db (prisma client library)
├─> notifications (Node.js library for sending notifications, part of scraper app)
└─> logger (Node.js library for logging, part of scraper app)
prisma
├─> migrations
└─> schema.prisma (database schema)
```

## Database

All data (metadata, embeddings, notifications, results) are stored in a [Postgres 15](https://www.postgresql.org/) database. The database schema is defined in [prisma/schema.prisma](https://github.com/Code42Cate/funding/blob/main/prisma/schema.prisma). To enable vector similarity search, we use [pgvector](https://github.com/pgvector/pgvector), an open-source Postgres extension for vector similarity search.

The following diagram shows the database schema which is extensively documented in [prisma/schema.prisma](https://github.com/Code42Cate/funding/blob/main/prisma/schema.prisma). There are 4 tables in total, one for the actual funding data. This table (`funding_oppportunities`) includes columns for metadata and the embeddings, with some extracted but nullable metadata columns such as `target_group` or `deadline_at`. The `jsonb` column meta contains all the unstructured data from the scraped data source. The `notifications` table stores the registered subscriptions, while `notification_history` stores the sent notifications for each subscription. The `results` are stored in the `results` table to enable caching and sharing of results.

<Image src="/db-schema.png" alt="Database Schema" width={500} height={500} />

## Web App

The Web app is a [Next.js](https://nextjs.org/) app that uses [Tailwind CSS](https://tailwindcss.com/). It is available [here](https://funding.nplusone.studio). It is a simple [search interface](/interface) that allows you to search for funding opportunities, and to subscribe to notifications or share results. Since we are using Next.js, it utilizes server-side rendering and includes API routes.

## Docs App

The Docs app is a [Next.js](https://nextjs.org/) app that uses [Nextra](https://nextra.site/), a site generation and documentation framework for Next.js. You are currently reading the docs app, available [here](https://docs.funding.nplusone.studio).

Before continuing, please follow the [Setup section](/setup) to get the docs app running on your own machine.

The structure of the docs app is quiete simple. There are two parts:

1. `_meta.json` file in the pages directory defines the routes and the navigation structure. For more information, see the [Nextra Docs](https://nextra.site/docs/guide/organize-files). This is a snippet of the `_meta.json` file:

```json
{
  "index": "Introduction",
  "contact": {
    "title": "Contact ↗",
    "type": "page",
    "href": "mailto:ubvja@student.kit.edu",
    "newWindow": true
  },
  "search": "Search",
  "notifications": "Notifications",
  "architecture": "Architecture"
}
```

Every key in the JSON object is a route and the value is either the title of the page or a JSON object with additional information.

2. MDX files in the pages directory. These files are the actual content of the docs app. For more information, see the [Nextra Docs](https://nextra.site/docs/guide/markdown). MDX is basically markddown, but you can also include some React components. This is a snippet of the `index.mdx` file:

```markdown
# About

This is the about page! **Markdown** [works](https://docs.funding.nplusone.studio/architecture) great!
```

## Scraper

The scraper has a fairly simple architecture. From a high level point of view, it consists of two parts. The first part is the actual scraping of the data, and the second the embedding of the funding data descriptions. These parts are separable, so you can run the scraper without the embedding part, or vice versa. This is done to allow manual quality control of the scraped data, and to allow for a more flexible embedding process. Especially needed because the OpenAI API [likes to crash](https://status.openai.com/) from time to time.

All Scrapers are run in parallel. This can be adapted if your network connection is not stable enough to handle multiple requests at once, but should be fine for most use cases.

After all scrapers have finished, the embeddings are generated. This is done in parallel as well, but with a throttle to not be rate limited by the OpenAI API. After all embeddings have been generated, the results are stored in the database.

The last step is to calculate and print the cost of the embeddings.

**TODO: NOTIFICATIONS**

<Image src="/scraper-flow.svg" alt="Scraper Flow" width={500} height={500} />

If you want to know how to run the scraper on your own, please refer to the [Setup section](/setup).
