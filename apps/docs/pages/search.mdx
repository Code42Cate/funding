import { Steps } from 'nextra-theme-docs';

# Search

This page describes how the search functionality works from a technical perspective. If you want to read about the user interface, please read the [Interface](/interface) page.

Whenever a user enters a search query, the following steps are performed:

<Steps>

### Embedding the query

The entered text is converted into an embedding vector with the [OpenAI text-embedding-ada-002 model](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings). A "Embedding Vector" is a list (vector) of numerical values (1536 to be exact) that represent a specific text. We do that because it is easier to compare two vectors than two texts. There are many ways to convert text into an embedding vector, but we use the OpenAI API because it is easy to use and works well. However, it is not free and also not open-source. An alternative would be [word2vec](https://arxiv.org/abs/1301.3781), although this would just trade API cost for compute cost, since we would have to do the inference ourselves.

### Finding the most similar vectors

Each funding opportunity that we have in our database also has an embedding vector that we create the same way as we did for the query. We then compare the query vector to each of the funding opportunity vectors and calculate the similarity. The similarity is a number between -1 and 1. We then sort the funding opportunities by their similarity to the query and show the top 10 results to the user. There are many ways to calculate the similarity between two vectors, but we use the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) because it is easy to calculate and works well. To make this calculation fast, we use the [pgvector](https://github.com/pgvector/pgvector) Postgres extension.

This looks something like this in SQL:

```sql
SELECT * FROM funding_opportunities ORDER BY embedding <-> ${your_embedding}::vector LIMIT 10
```

### Showing the best (summarized) results

The previous step returns the top 10 results, and we show each result on a separate page. But just displaying the entire description as previously scraped is often not very helpful. Since these are legal documents, they are usually written in a very [formal and verbose language](http://web.archive.org/web/20230619071641/https://ec.europa.eu/info/funding-tenders/opportunities/portal/screen/opportunities/topic-details/edf-2023-ls-ra-dis-nt;callCode=null;freeTextSearchKeyword=;matchWholeText=true;typeCodes=0,1,2,8;statusCodes=31094501,31094502,31094503;programmePeriod=null;programCcm2Id=44181033;programDivisionCode=null;focusAreaCode=null;destinationGroup=null;missionGroup=null;geographicalZonesCode=null;programmeDivisionProspect=null;startDateLte=null;startDateGte=null;crossCuttingPriorityCode=null;cpvCode=null;performanceOfDelivery=null;sortQuery=sortStatus;orderBy=asc;onlyTenders=false;topicListKey=topicSearchTablePageState). To solve this and make the results more readable (and skimmable), we use the [OpenAI text-davinci-003 model](https://platform.openai.com/docs/models/gpt-3-5). This model takes a long text and returns a short summary of it[^1]. We then show this summary to the user. In order to show _something_ immediately, we stream the result. This means that as the next word of the summary is available it gets shown and we do not wait for the entire summarization before showing it. To better understand how this works, read the [Interface](/interface) page.

[^1]: It can do a lot more, but this is what we instructed it to do with our prompt.

</Steps>
